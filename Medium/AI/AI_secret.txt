This embarrassingly simple secret explains all of AI.
04-Jan-2026

https://ai.gopubby.com/stop-learning-ai-backwards-e20bd7d7d2cb

Every single AI model can be seen as a probability problem.

Probability is literally the foundation that the whole field of AI stands on.


Sovling for p
Anyway, to optimise any function, we take the derivative and set it to zero.

p equals the average of all the xᵢ values.



LLM
An LLM is an "input-conditioned machine".

Training an LLM = estimating a distribution


Big picture
When you interact with ChatGPT or Claude, you’re essentially querying a massive probability distribution.


Conclusion
When an LLM predicts the next word, it’s not doing anything magical. 
It’s estimating probability distributions that best explain the training data it’s seen.
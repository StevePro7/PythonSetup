If You Can Answer These GenAI Questions, You’re Already Hireable
01-Jan-2026


https://medium.com/activated-thinker/if-you-can-answer-these-genai-questions-youre-already-hireable-9e96db97e83c


Q1. How do Large Language Models (LLMs) actually work under the hood?
A1:
An LLM is a neural network trained to predict the next token based on the 
previous tokens using the Transformer architecture


Remember
Tokens are basic units of text that a model reads, processes, and generates


Deep understanding
LLM does one task well:
given previous text predict the mostlikely next token


Input text = tokens
Tokens converted into embeddings
Embeddings pass thru many Transformer layers
Each layer improves context understanding
Final layer outputs probability distribution over next tokens


LLMs
model statistical patterns so well that understanding appears


IMPORTANT
LLM training	self-supervised
Loss function	cross entropy


Q2. What problem did the Transformer architecture solve, and why did it replace RNNs/LSTMs?
A2:
Problem with RNNs/LSTMs:
* Sequential processing (slow)
* Hard to scale
* Weak long-range memory
* Vanishing gradients problem

Transformers
* Parallel processing [read entire sequence at once]
* Each word can look at every other word in the sentence
  [via attention - global context]
* Better scaling on GPUs


IMPORTANT
replace recurrent with attention

Transformers
* faster training
* better context handling
* massive scalability		LLMs possible

Transformers
made it feasible to train LLM with billions of parameters


Q3. How does self-attention work, and why is it so powerful?
A3:
self-attention allows the model to understand - what is "it"

NB:
each token creates:
Q	Query	what am I looking for?
K	Key	what do I offer?
V	Value	what information do I pass if selected??


Multi-attention
multiple attention heads to focus on different aspects:
* syntax
* semantics
* position
* long-range relations

INSIGHT
Self-attention allows models to dynamically focus on relevant parts of the context


Q4. What is tokenization, and how does it affect cost, context length, and accuracy?
A4:
Tokenization
converting text into model-understandable units [tokens]

NB:
subword tokenization
"unbelievable" → ["un", "believ", "able"]

INSIGHT
Tokenization directly impacts performance, latency, and cost


Q5. What happens inside GPT when you enter a prompt? (End-to-End Flow)
A5:
Full pipeline
1. user input		tokenizer
2. tokens -> embeddings
3. positional encoding added		Helps model understand position or words
4. pass thru N Transformer layers
5. logits generated	raw unnormalized scores	help in deciding which tokens to select
6. sampling	temperature, top-p	decides how the next token is chosen
7. token selected
8. repeat until stop


Remember
logit
raw unnormalized scores for each possible next token


IMPORTANT
* GPT is decoder-only	only generates
* GPT has no encoder like BERT
* Autoregressive generation	model generates one token at a time
				each new token added back to the input


INSIGHT
GPT generates text one token at a time conditioning each step on all previous tokens


REVISION
* LLM = next-token predictor
* Transformers enable scale
* Attention = context understanding
* Tokenization affects everything
* GPT = autoregressive decoder


Q6. What are embeddings, and how do they represent semantic meaning?
A6:
Embedding [definition]
An embedding is a list of numbers [a vector] that represents the meaning of text

Embeddings
convert meaning into geometry


During training
* model sees billions of sentences
* learns that certain words appear in similar contexts

IDEA
words with similar meanings appear in similar contexts
model places them close together in vector space


Embeddings = powerful
* compare meaning	semantic search
* retrieve relevant documents
* cluster similar content
* create RAG

NB:
Embeddings
allow semantic understanding NOT keyword matching

INSIGHT
Embeddings are dense vector representations of text that encode semantic
meaning allowing similarity-based retrieval


Q7. Why do we need a vector database? Why not SQL or Elasticsearch?
A7:
Find most relevant documents for this query

SQL is not built for geometry
SQL is built for structured rows and columns

Elasticsearch not optimized for large-scale ANN	[Approximate Nearest Neighbor]
Elasticsearch has higher latency and less control over vector indexing

Vector DBs optimized
* high-dimensional vectors	[no. elements in a vector]
* Approximate Nearest Neighbor	ANN
* fast similarity lookup
* metadata filtering

e.g.
Chroma
FAISS
Pinecone
Weaviate

Vector DBs = search engines for meaning

INSIGHT
SQL 		is for structured data
Elasticsearch	is for keyword relevance
Vector DBs	are for semantic similarity at scale


Q8. How does similarity search work in vector databases (cosine, ANN, HNSW)?
A8:
Similarity search
does this document mean the same thing?

Cosine similarity
angle btwn two vectors
 1.0	identical meaning
 0.0	no relation
-1.0	opposite meaning

Nearest neighbor search = too slow

ANN
Approximate Nearest Neighbor
trades tiny accuracy loss for massive speed gain
only searches likely candidates

HNSW
Hierarchical Navigable Small World graph
ANN algorithm

graph of vectors
similar vectors are connected
multiple layers	coarse -> fine

HNSW
makes vector search fast without scanning everything
that is why it is widely used

INSIGHT
Vector DBs use ANN algorithms like HNSW to efficiently find
semantically similar embeddings without exact comparison


REVISION
* Embeddings = meanings -> numbers
* Similar meaning = closer vectors
* SQL != semantic search
* Vectors DBs = optimized for ANN	Approximate Nearest Neighbors
* Cosine similarity = angle between meanings
* ANN = fast + scalable
* HNSW = graph-based retrieval


Q9. What is RAG, and why is it preferred over fine-tuning in many systems?
A9.
LLMs = 3x issues
1. lack of new knowledge
2. hallucinations	[guess when unsure]
3. no knowledge about private documents

RAG
fixes this by inserting external knowledge at runtime

Retrieval Augmented Generation
* retrieve relevant documents
* give them [docs] to the LLM
* generate an answer based on those documents


PROCESS
1. user asks question
2. question -> embedding
3. search vector DB		here knowledge embeddings are present
4. retrieve top-k documents	ones similar to query answer
5. inject documents + query into prompt and give it to LLM
6. LLM generates answer using this context


RAG
no retraining
data stays freshj
lower cost
easier updates
fewer hallucinations

BUT
expensive
slow to update
hard to debug
not good for changing knowledge

Rule of thumb
use RAG for knowledge fine-tuning behavior

INSIGHT
RAG is preferred because it injects up-to-date external knowledge
without retraining the model


Q10. What are the most common failure points in a RAG pipeline?
A10:
1. Bad chunking
chunks too small	lose context
chunks too large	irrelevant info
chunking quality directly affects answer quality

2. Embeddings mismatch
using wrong embedding model
domain mismatch
retriever fails silently

3. Poor retrieval strategy
top-k too low		miss info
top-k too high		noise
no metadata filtering
LLM overwhelmed or misled

4. Context overload
too many tokens in prompt
more context != better answers

5. Weak prompt
without grounding hallucinations still happen

6. LLM limitations
ever with perfect retrieval
model may give wrong reasoning
model may overgeneralize
RAG reduces hallunication but doesn't eliminate it

RAG
improves information availability not reasoning correctness


INSIGHTS
* RAG injects external knowledge at inference time
* Most RAG failures come from chunking and retrieval NOT the LLM
* RAG reduces hallucinations but doesn't guarantee correctness


Q11. What is fine-tuning, and when should you NOT use it?
A11:
Fine-tuning
* taking a pretrained LLM
* updating its weights using task-specific data

changes how models behaves NOT what it knows
Fine-tuning is not a knowledge update mechanism

RULE
use RAG		for knowledge
use fine-tuning for behavior


Q12. What is LoRA and why is it important?
A12:
tradition fine-tuning issues
* updating billions parameters
* huge GPU memory
* very expensive

LoRA = Low-Rank Adaption
* freeze the base model
* add small trainable matrices
* train only those

modify behavior without touching core model
almost ALL modern fine-tuning uses LoRA

INSIGHT
LoRA enables efficient fine-tuning by updating low-rank adapters
instead of full model weights


Q13. How do you reduce hallucinations in GenAI systems?
A13:
You cannot eliminate hallucinations. You can only reduce them

Hallucinations
* missing context
* ambiguous questions
* model overconfidence
* weak prompts

Mitigation strategies
1. Retrieval grounding
2. Strong system prompts
3. Smaller focuses context
4. Output validation
5. Human-in-the-loop		critical systems

INSIGHTS
Hallucinations are reduced by grounding not by prompting alone


Q14. What do temperature, top-k, and top-p control?
A14:
control how the next token is chosen
e.g.
Temperature
Top-k
Top-p		nucleus sampling
Practical rule

INSIGHT
Sampling parameters balance determinism and diversity during generation


Q15. How do you evaluate GenAI systems in production?
A15:
Evaluation depends on the use case
e.g.
RAG systems
1. Retrieval quality
2. Grounding / faithfulness
3. Final answer quality

Production metrics
* latency
* token cost
* error rate
* user feedback		which answer is more helpful or clearer

GenAI evaluation is system-level NOT model-only

INSIGHT
GenAI evaluation requires combining retrieval metrics, generation quality and
human judgement


REVISION
* LLM		= next-token predictor
* Transformer enables scale
* Attention	= context selection
* Embeddings	= meaning -> vectors
* Vector DB 	= semantic search
* ANN + HNSW 	= fast retrieval
* RAG		= knowledge injection
* Chunking matters more than models
* Fine-tuning	= behavior change
* LoRA		= efficient fine-tuning
* Hullucinations need grounding
* Sampling controls creativity
* Evaluation is multi-layered
* Production	= cost + latency + safety 

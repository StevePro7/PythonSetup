Output
Math for ML: The Essential Foundations for Understanding Deep Learning
https://medium.com/@ryassminh/math-for-ml-the-essential-foundations-for-understanding-deep-learning-950e37daf0f3
04-Dec-2025

Ex00
Linear output (z): 1.1
Neuron output (a): 0.7502601055951177


Ex01
Sigmoid function
squashes numbers into the range (0, 1)


Ex02 Tanh
Hyperbolic Tangent
Tanh looks like sigmoid but shifted to (-1, 1)


Ex03 ReLU
Rectified Linear Unit
like a light switch:
nothing passes if the input is negative
but as soon as it's positive it flows freely


Ex04
Leaky ReLU
allows small negative outputs so neurons don't completely "die"


Ex05 GEUL
Gaussian Error Linear Unit


Ex06
[0.65900114 0.24243297 0.09856589]


Ex07
Activation Function Comparison
Validation Loss
Validation Accuracy


Ex08
None


Ex09
Moderate Scale Shift
Extreme Scale Shift


Ex10
=== Training Plain MLP ===
Epoch 1: Train Loss=0.346, Acc=0.904, Val Loss=0.151, Acc=0.955
Epoch 2: Train Loss=0.132, Acc=0.960, Val Loss=0.108, Acc=0.966
Epoch 3: Train Loss=0.087, Acc=0.974, Val Loss=0.088, Acc=0.974
Epoch 4: Train Loss=0.065, Acc=0.981, Val Loss=0.077, Acc=0.974
Epoch 5: Train Loss=0.050, Acc=0.984, Val Loss=0.080, Acc=0.977

=== Training MLP with BatchNorm ===
Epoch 1: Train Loss=0.222, Acc=0.942, Val Loss=0.088, Acc=0.974
Epoch 2: Train Loss=0.077, Acc=0.977, Val Loss=0.074, Acc=0.976
Epoch 3: Train Loss=0.051, Acc=0.984, Val Loss=0.065, Acc=0.979
Epoch 4: Train Loss=0.036, Acc=0.990, Val Loss=0.068, Acc=0.978
Epoch 5: Train Loss=0.029, Acc=0.991, Val Loss=0.061, Acc=0.980


Ex11
LayerNorm
=== Training Plain RNN (no normalization) ===
Epoch  1: Train Loss=0.663, Acc=0.613 | Val Loss=0.681, Acc=0.588
Epoch  2: Train Loss=0.657, Acc=0.611 | Val Loss=0.587, Acc=0.689
Epoch  3: Train Loss=0.652, Acc=0.625 | Val Loss=0.574, Acc=0.741
Epoch  4: Train Loss=0.652, Acc=0.615 | Val Loss=0.593, Acc=0.693
Epoch  5: Train Loss=0.629, Acc=0.648 | Val Loss=0.589, Acc=0.704
Epoch  6: Train Loss=0.668, Acc=0.605 | Val Loss=0.662, Acc=0.615
Epoch  7: Train Loss=0.658, Acc=0.609 | Val Loss=0.593, Acc=0.710
Epoch  8: Train Loss=0.657, Acc=0.610 | Val Loss=0.732, Acc=0.506
Epoch  9: Train Loss=0.695, Acc=0.518 | Val Loss=0.687, Acc=0.511
Epoch 10: Train Loss=0.690, Acc=0.544 | Val Loss=0.682, Acc=0.559
Epoch 11: Train Loss=0.687, Acc=0.557 | Val Loss=0.679, Acc=0.554
Epoch 12: Train Loss=0.684, Acc=0.565 | Val Loss=0.677, Acc=0.554
Epoch 13: Train Loss=0.676, Acc=0.576 | Val Loss=0.647, Acc=0.616
Epoch 14: Train Loss=0.674, Acc=0.575 | Val Loss=0.681, Acc=0.559
Epoch 15: Train Loss=0.682, Acc=0.567 | Val Loss=0.667, Acc=0.583

=== Training LayerNorm RNN ===
Epoch  1: Train Loss=0.669, Acc=0.595 | Val Loss=0.676, Acc=0.578
Epoch  2: Train Loss=0.673, Acc=0.573 | Val Loss=0.673, Acc=0.566
Epoch  3: Train Loss=0.659, Acc=0.620 | Val Loss=0.653, Acc=0.621
Epoch  4: Train Loss=0.638, Acc=0.638 | Val Loss=0.642, Acc=0.632
Epoch  5: Train Loss=0.543, Acc=0.727 | Val Loss=0.422, Acc=0.810
Epoch  6: Train Loss=0.442, Acc=0.802 | Val Loss=0.291, Acc=0.865
Epoch  7: Train Loss=0.347, Acc=0.842 | Val Loss=0.406, Acc=0.820
Epoch  8: Train Loss=0.425, Acc=0.799 | Val Loss=0.666, Acc=0.568
Epoch  9: Train Loss=0.607, Acc=0.668 | Val Loss=0.723, Acc=0.524
Epoch 10: Train Loss=0.607, Acc=0.681 | Val Loss=0.599, Acc=0.731
Epoch 11: Train Loss=0.606, Acc=0.680 | Val Loss=0.668, Acc=0.586
Epoch 12: Train Loss=0.676, Acc=0.578 | Val Loss=0.666, Acc=0.604
Epoch 13: Train Loss=0.672, Acc=0.591 | Val Loss=0.662, Acc=0.611
Epoch 14: Train Loss=0.669, Acc=0.594 | Val Loss=0.657, Acc=0.611
Epoch 15: Train Loss=0.664, Acc=0.594 | Val Loss=0.650, Acc=0.625


Ex12
GroupNorm vs. BatchNorm
[bn][epoch 1] train_acc=0.287 val_acc=0.371
[gn][epoch 1] train_acc=0.324 val_acc=0.408
[bn][epoch 1] train_acc=0.324 val_acc=0.374
[gn][epoch 1] train_acc=0.316 val_acc=0.355
[bn][epoch 1] train_acc=0.360 val_acc=0.384
[gn][epoch 1] train_acc=0.320 val_acc=0.384
[bn][epoch 1] train_acc=0.370 val_acc=0.359
[gn][epoch 1] train_acc=0.304 val_acc=0.368